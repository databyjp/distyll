{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Workshop demo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize Weaviate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 6666\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import weaviate\n",
    "from weaviate import EmbeddedOptions\n",
    "\n",
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions(version=\"latest\"),\n",
    "    additional_headers={\"X-OpenAI-Api-Key\": os.environ[\"OPENAI_APIKEY\"]}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T19:31:26.623553Z",
     "start_time": "2023-07-20T19:31:26.111963Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.is_ready()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:44.351369Z",
     "start_time": "2023-07-20T14:05:44.346463Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Clean slate\n",
    "client.schema.delete_all()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:44.355151Z",
     "start_time": "2023-07-20T14:05:44.352078Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the collection (class)\n",
    "\n",
    "Add class name & specify vectorizer & generative modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class_definition = {\n",
    "    \"class\": \"Chunk\",\n",
    "    \"vectorizer\": \"text2vec-openai\",\n",
    "    \"moduleConfig\": {\n",
    "        \"generative-openai\": {}\n",
    "    },\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:44.357165Z",
     "start_time": "2023-07-20T14:05:44.355541Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"chunk_1Ur6HqTJ4I1E\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-07-20T15:05:44+01:00\",\"took\":38583}\n"
     ]
    }
   ],
   "source": [
    "client.schema.create_class(class_definition)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:44.392967Z",
     "start_time": "2023-07-20T14:05:44.359205Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Confirm class creation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'class': 'Chunk',\n 'invertedIndexConfig': {'bm25': {'b': 0.75, 'k1': 1.2},\n  'cleanupIntervalSeconds': 60,\n  'stopwords': {'additions': None, 'preset': 'en', 'removals': None}},\n 'moduleConfig': {'generative-openai': {},\n  'text2vec-openai': {'model': 'ada',\n   'modelVersion': '002',\n   'type': 'text',\n   'vectorizeClassName': True}},\n 'multiTenancyConfig': {'enabled': False},\n 'properties': [],\n 'replicationConfig': {'factor': 1},\n 'shardingConfig': {'virtualPerPhysical': 128,\n  'desiredCount': 1,\n  'actualCount': 1,\n  'desiredVirtualCount': 128,\n  'actualVirtualCount': 128,\n  'key': '_id',\n  'strategy': 'hash',\n  'function': 'murmur3'},\n 'vectorIndexConfig': {'skip': False,\n  'cleanupIntervalSeconds': 300,\n  'maxConnections': 64,\n  'efConstruction': 128,\n  'ef': -1,\n  'dynamicEfMin': 100,\n  'dynamicEfMax': 500,\n  'dynamicEfFactor': 8,\n  'vectorCacheMaxObjects': 1000000000000,\n  'flatSearchCutoff': 40000,\n  'distance': 'cosine',\n  'pq': {'enabled': False,\n   'bitCompression': False,\n   'segments': 0,\n   'centroids': 256,\n   'trainingLimit': 100000,\n   'encoder': {'type': 'kmeans', 'distribution': 'log-normal'}}},\n 'vectorIndexType': 'hnsw',\n 'vectorizer': 'text2vec-openai'}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.schema.get(\"Chunk\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:44.393270Z",
     "start_time": "2023-07-20T14:05:44.375621Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data\n",
    "\n",
    "From a text file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'A large language model (LLM) is a computerized language model, embodied by an artificial neural netw'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"srcdata/llm_wiki.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:44.393347Z",
     "start_time": "2023-07-20T14:05:44.387901Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "How long is it?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "39108"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:44.393411Z",
     "start_time": "2023-07-20T14:05:44.388080Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "7206"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split(\" \"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:44.393477Z",
     "start_time": "2023-07-20T14:05:44.388114Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Chunk it (e.g. using paragraph markers or number of words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model (LLM) is a computerized lan ...\n",
      "=====\n",
      "\n",
      "History\n",
      "Precursors\n",
      "The basic idea of LLMs, which i ...\n",
      "=====\n",
      "\n",
      "Lead-up to the transformer framework\n",
      "The earliest  ...\n",
      "=====\n",
      "\n",
      "The seq2seq model (380 million parameters) used tw ...\n",
      "=====\n",
      "\n",
      "BERT and GPT\n",
      "While there are many models with diff ...\n",
      "=====\n",
      "\n",
      "Origin of the term and disambiguation\n",
      "While the te ...\n",
      "=====\n",
      "\n",
      "Linguistic foundations\n",
      "Cognitive Linguistics offer ...\n",
      "=====\n",
      "\n",
      "Architecture\n",
      "Large language models have most commo ...\n",
      "=====\n",
      "\n",
      "Tokenizers, which convert text into machine-readab ...\n",
      "=====\n",
      "\n",
      "Tokenization\n",
      "LLMs are mathematical functions whose ...\n",
      "=====\n",
      "\n",
      "Output\n",
      "The output of a LLM is a probability distri ...\n",
      "=====\n",
      "\n",
      "Upon receiving a text, the bulk of the LLM outputs ...\n",
      "=====\n",
      "\n",
      "Context window\n",
      "The context window of a LLM is the  ...\n",
      "=====\n",
      "\n",
      "Training\n",
      "In the pre-training, LLMs may be trained  ...\n",
      "=====\n",
      "\n",
      "autoregressive (i.e. predicting how the segment co ...\n",
      "=====\n",
      "\n",
      "Dataset size and compression\n",
      "In 2018, the BookCorp ...\n",
      "=====\n",
      "\n",
      "Training cost\n",
      "Advances in software and hardware ha ...\n",
      "=====\n",
      "\n",
      "Application to downstream tasks\n",
      "Between 2018 and 2 ...\n",
      "=====\n",
      "\n",
      "From fine-tuning to prompting\n",
      "The old approach was ...\n",
      "=====\n",
      "\n",
      "Review: This movie is fantastic!\n",
      "Sentiment: ...\n",
      "=====\n",
      "\n",
      "If the model outputs \"positive\", then it has corre ...\n",
      "=====\n",
      "\n",
      "Instruction tuning\n",
      "Often, instruction tuning is ne ...\n",
      "=====\n",
      "\n",
      "Finetuning by reinforcement learning\n",
      "OpenAI's Inst ...\n",
      "=====\n",
      "\n",
      "Tool use\n",
      "There are certain tasks that, in principl ...\n",
      "=====\n",
      "\n",
      "Agency\n",
      "An LLM is a language model, which is not an ...\n",
      "=====\n",
      "\n",
      "Multimodality\n",
      "Multimodality means \"having several  ...\n",
      "=====\n",
      "\n",
      "Properties\n",
      "Pretraining datasets\n",
      "Large language mod ...\n",
      "=====\n",
      "\n",
      "Scaling laws and emergent abilities\n",
      "The following  ...\n",
      "=====\n",
      "\n",
      "cost of (pre-)training (\n",
      "  \n",
      "    C\n",
      "    C\n",
      "  ),\n",
      "size  ...\n",
      "=====\n",
      "\n",
      "  \n",
      "    C\n",
      "    C\n",
      "   is the cost of training the mode ...\n",
      "=====\n",
      "\n",
      "  \n",
      "    N\n",
      "    N\n",
      "   is the number of parameters in t ...\n",
      "=====\n",
      "\n",
      "  \n",
      "    D\n",
      "    D\n",
      "   is the number of tokens in the t ...\n",
      "=====\n",
      "\n",
      "  \n",
      "    L\n",
      "    L\n",
      "   is the average negative log-like ...\n",
      "=====\n",
      "\n",
      "  \n",
      "    \n",
      "      \n",
      "        \n",
      "          C\n",
      "          \n",
      "    ...\n",
      "=====\n",
      "\n",
      "  \n",
      "    \n",
      "      \n",
      "        Œ±\n",
      "        =\n",
      "        0.34\n",
      "   ...\n",
      "=====\n",
      "\n",
      "reported arithmetics, decoding the International P ...\n",
      "=====\n",
      "\n",
      "Interpretation\n",
      "Large language models by themselves ...\n",
      "=====\n",
      "\n",
      "Understanding and intelligence\n",
      "NLP researchers wer ...\n",
      "=====\n",
      "\n",
      "Evaluation\n",
      "Perplexity\n",
      "The most commonly used measu ...\n",
      "=====\n",
      "\n",
      "Task-specific datasets and benchmarks\n",
      "A large numb ...\n",
      "=====\n",
      "\n",
      "Adversarially constructed evaluations\n",
      "Because of t ...\n",
      "=====\n",
      "\n",
      "We see a fitness center sign. We then see a man ta ...\n",
      "=====\n",
      "\n",
      "BERT selects b) as the most likely completion, tho ...\n",
      "=====\n",
      "\n",
      "Wider impact\n",
      "In 2023, Nature Biomedical Engineerin ...\n",
      "=====\n",
      "\n",
      "List\n",
      "Further reading\n",
      "Jurafsky, Dan, Martin, James. ...\n",
      "=====\n",
      "\n",
      "See also\n",
      "Foundation models\n",
      "Generative AI ...\n",
      "=====\n",
      "\n",
      "Notes ...\n",
      "=====\n",
      "\n",
      "\n",
      "== References == ...\n",
      "=====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = text.split(\"\\n\\n\")\n",
    "for chunk in chunks:\n",
    "    print(chunk[:50], \"...\\n=====\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:44.393677Z",
     "start_time": "2023-07-20T14:05:44.389167Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import data - chunk by chunk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "with client.batch() as batch:\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        weaviate_object = {\n",
    "            \"body\": chunk,\n",
    "            \"chunk_no\": i,\n",
    "            \"source\": \"llm_wiki.txt\",\n",
    "        }\n",
    "        batch.add_data_object(\n",
    "            class_name=\"Chunk\",\n",
    "            data_object=weaviate_object,\n",
    "            uuid=generate_uuid5(weaviate_object)\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:47.236394Z",
     "start_time": "2023-07-20T14:05:44.393119Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query time!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get object count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{'data': {'Aggregate': {'Chunk': [{'meta': {'count': 48}}]}}}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.query.aggregate(\"Chunk\").with_meta_count().do()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:47.241916Z",
     "start_time": "2023-07-20T14:05:47.236883Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "48"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:47.245207Z",
     "start_time": "2023-07-20T14:05:47.242250Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform a semantic search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "res = (\n",
    "    client.query.get(\"Chunk\", [\"body\", \"chunk_no\"])\n",
    "    .with_near_text(\n",
    "        {\"concepts\": [\"history of large language models\"]}\n",
    "    )\n",
    "    .with_limit(3)\n",
    "    .do()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:47.786322Z",
     "start_time": "2023-07-20T14:05:47.247444Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": {\n",
      "    \"Get\": {\n",
      "      \"Chunk\": [\n",
      "        {\n",
      "          \"body\": \"Lead-up to the transformer framework\\nThe earliest \\\"large\\\" language models were built with recurrent architectures such as the long short-term memory (LSTM) (1997). After AlexNet (2012) demonstrated the effectiveness of large neural networks for image recognition, researchers applied large neural networks to other tasks. In 2014, two main techniques were proposed.\",\n",
      "          \"chunk_no\": 2\n",
      "        },\n",
      "        {\n",
      "          \"body\": \"Origin of the term and disambiguation\\nWhile the term of Large Language Models has itself emerged around 2018, it gained visibility in 2019 and 2020, with the release of DistilBERT and Stochastic Parrots papers respectively. Both focused on the \\\"Large-scale pretrained models\\\", citing as an example of LLMs the BERT family, starting at 110M parameters and referring to models in the 340M parameters range as \\\"very large LMs\\\".\\nPerhaps surprisingly, both cite the pre-transformer RNN-based ELMo - the 2018 architecture that inspired BERT - as the first LLM, given the number of parameters (94M), as well as the size of the pretraining dataset (>1B tokens). Despite the comparable parameter size, the original Transformer is generally not considered as an LLM due to a smaller pretraining dataset (generally estimated in the 100M tokens range).\\nOverall, due to a smooth scaling in LLM model performance from  ~100M parameters to 500B+ parameters and progressive unlocking of emergent capabilities such as multi-lingual translation, arithmetic, or programming code composition, all post-ELMo models are referred to by researchers as LLMs.\",\n",
      "          \"chunk_no\": 5\n",
      "        },\n",
      "        {\n",
      "          \"body\": \"List\\nFurther reading\\nJurafsky, Dan, Martin, James. H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 3rd Edition draft, 2023.\\nPhuong, Mary; Hutter, Marcus (2022). \\\"Formal Algorithms for Transformers\\\". arXiv:2207.09238 [cs.LG].\\nEloundou, Tyna; Manning, Sam; Mishkin, Pamela; Rock, Daniel (2023). \\\"GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models\\\". arXiv:2303.10130 [econ.GN].\\nEldan, Ronen; Li, Yuanzhi (2023). \\\"TinyStories: How Small Can Language Models Be and Still Speak Coherent English?\\\". arXiv:2305.07759 [cs.CL].\\nFrank, Michael C. (27 June 2023). \\\"Baby steps in evaluating the capacities of large language models\\\". Nature Reviews Psychology: 1\\u20132. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023.\\nZhao, Wayne Xin;  et al. (2023). \\\"A Survey of Large Language Models\\\". arXiv:2303.18223 [cs.CL].\",\n",
      "          \"chunk_no\": 44\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(res, indent=2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:05:47.789272Z",
     "start_time": "2023-07-20T14:05:47.786822Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform a generative search - e.g. explain the history of large langauge models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "res = (\n",
    "    client.query.get(\"Chunk\", [\"body\", \"chunk_no\"])\n",
    "    .with_near_text(\n",
    "        {\"concepts\": [\"history\"]}\n",
    "    )\n",
    "    .with_limit(5)\n",
    "    .with_generate(\n",
    "        grouped_task=\"Explain the history of large language models in plain language, based on this text\"\n",
    "    )\n",
    "    .do()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:06:03.817802Z",
     "start_time": "2023-07-20T14:05:47.790061Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "{'data': {'Get': {'Chunk': [{'_additional': {'generate': {'error': None,\n       'groupedResult': 'Large language models (LLMs) have a history that dates back to the 1950s. However, the idea of using neural networks as black boxes with randomized weights to train on a large language corpus was not feasible until the 2010s. This was because the use of GPUs (graphics processing units) enabled massively parallelized processing, which replaced the previous logical AI approach that relied on symbolic programs.\\n\\nBefore LLMs, there were precursors that paved the way for their development. One such precursor was the Elman network, which was a recurrent network trained on simple sentences like \"dog chases man.\" The trained model was then used to convert each word into a vector, known as its \\'internal representation.\\' These vectors were clustered based on their closeness, forming a tree-like structure. Within this structure, verbs and nouns belonged to one large cluster, and within the noun cluster, there were further divisions into inanimates and animates.\\n\\nIn the 1950s, the idea of learning natural language through a simple repetitive architecture remained just an idea due to the lack of modern GPUs for parallel processing. However, in the 1990s, the IBM alignment models for statistical machine translation hinted at the future success of LLMs. It was in 2001 that an early work used a 1-billion-word corpus scraped from the internet for word disambiguation, distinguishing words like \"then\" and \"than.\" This work was considered significant as it utilized a large corpus, which was considered huge at that time.\\n\\nOverall, the history of large language models involves the development of neural networks, the use of GPUs for parallel processing, and the exploration of different techniques to train models on large language corpora.'}},\n     'body': '\\n== References ==',\n     'chunk_no': 47},\n    {'_additional': {'generate': None},\n     'body': 'Review: This movie is fantastic!\\nSentiment:',\n     'chunk_no': 19},\n    {'_additional': {'generate': None}, 'body': 'Notes', 'chunk_no': 46},\n    {'_additional': {'generate': None},\n     'body': 'BERT selects b) as the most likely completion, though the correct answer is d).',\n     'chunk_no': 42},\n    {'_additional': {'generate': None},\n     'body': 'History\\nPrecursors\\nThe basic idea of LLMs, which is to start with a neural network as black box with randomized weights, using a simple repetitive architecture and (pre-)training it on a large language corpus, was not feasible until the 2010s when use of GPUs had enabled massively parallelized processing, which has gradually replaced the logical AI approach that has relied on symbolic programs.Precursors of LLMs included the Elman network, in which a recurrent network was trained on simple sentences like \"dog chases man\". Then, the (pre-)trained model was used to convert each word into a vector (its \\'internal representation\\'). These vectors were clustered by closeness into a tree. The tree was then found to have a structure. The verbs and nouns each belonged to one large cluster. Within the noun cluster, there are two clusters: inanimates and animates. And so on.\\nIn the 1950s, without the modern GPUs enabling massively parallel processing, the idea to learn natural language by a simple repetitive architecture remained just an idea. Later in 1990s, the IBM alignment models for statistical machine translation announced the future success of LLMs. An early work that uses corpus scraped from the Internet for word disambiguation (such as distinguishing \"then\" and \"than\") in 2001. It used a 1-billion-word corpus, considered huge at the time.',\n     'chunk_no': 1}]}}}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:06:03.823316Z",
     "start_time": "2023-07-20T14:06:03.818986Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "'Large language models (LLMs) have a history that dates back to the 1950s. However, the idea of using neural networks as black boxes with randomized weights to train on a large language corpus was not feasible until the 2010s. This was because the use of GPUs (graphics processing units) enabled massively parallelized processing, which replaced the previous logical AI approach that relied on symbolic programs.\\n\\nBefore LLMs, there were precursors that paved the way for their development. One such precursor was the Elman network, which was a recurrent network trained on simple sentences like \"dog chases man.\" The trained model was then used to convert each word into a vector, known as its \\'internal representation.\\' These vectors were clustered based on their closeness, forming a tree-like structure. Within this structure, verbs and nouns belonged to one large cluster, and within the noun cluster, there were further divisions into inanimates and animates.\\n\\nIn the 1950s, the idea of learning natural language through a simple repetitive architecture remained just an idea due to the lack of modern GPUs for parallel processing. However, in the 1990s, the IBM alignment models for statistical machine translation hinted at the future success of LLMs. It was in 2001 that an early work used a 1-billion-word corpus scraped from the internet for word disambiguation, distinguishing words like \"then\" and \"than.\" This work was considered significant as it utilized a large corpus, which was considered huge at that time.\\n\\nOverall, the history of large language models involves the development of neural networks, the use of GPUs for parallel processing, and the exploration of different techniques to train models on large language corpora.'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"data\"][\"Get\"][\"Chunk\"][0][\"_additional\"][\"generate\"][\"groupedResult\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:06:03.826615Z",
     "start_time": "2023-07-20T14:06:03.822632Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Can we make it snappier & more interesting?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "res = (\n",
    "    client.query.get(\"Chunk\", [\"body\", \"chunk_no\"])\n",
    "    .with_near_text(\n",
    "        {\"concepts\": [\"history\"]}\n",
    "    )\n",
    "    .with_limit(5)\n",
    "    .with_generate(\n",
    "        grouped_task=\"Explain the history of large language models in a tweet with emojis, based on the following text\"\n",
    "    )\n",
    "    .do()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:06:07.919835Z",
     "start_time": "2023-07-20T14:06:03.827061Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "'üìöüîç LLMs history: In the 1950s, the idea of using a simple repetitive architecture to learn natural language was just an idea. In the 1990s, IBM alignment models paved the way for LLMs. In the 2010s, GPUs enabled massively parallel processing, making LLMs feasible. üñ•Ô∏èüí°üî¢üåê #LLM #history'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"data\"][\"Get\"][\"Chunk\"][0][\"_additional\"][\"generate\"][\"groupedResult\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:06:07.926245Z",
     "start_time": "2023-07-20T14:06:07.921483Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T14:06:07.929215Z",
     "start_time": "2023-07-20T14:06:07.926624Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
