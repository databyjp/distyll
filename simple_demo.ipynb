{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Demo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instantiate database\n",
    "\n",
    "You can connect to your database like this."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 6666\n"
     ]
    }
   ],
   "source": [
    "import distyll\n",
    "db = distyll.DBConnection()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T02:09:57.895810Z",
     "start_time": "2023-09-03T02:09:57.182945Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optionally, you can also specify a particular Weaviate instance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# import weaviate\n",
    "# import os\n",
    "# client = weaviate.Client(\n",
    "#     url=os.environ['JP_WCS_URL'],\n",
    "#     auth_client_secret=weaviate.AuthApiKey(os.environ['JP_WCS_ADMIN_KEY']),\n",
    "#     additional_headers={\n",
    "#         'X-OpenAI-Api-Key': os.environ['OPENAI_APIKEY']\n",
    "#     }\n",
    "# )\n",
    "#\n",
    "# import distyll\n",
    "# db = distyll.DBConnection(client=client)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T02:09:57.898763Z",
     "start_time": "2023-09-03T02:09:57.897219Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## YouTube video example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's add data from a YouTube video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "youtube_url = \"https://youtu.be/sNw40lEhaIQ\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T02:09:57.900629Z",
     "start_time": "2023-09-03T02:09:57.899406Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "52"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.add_from_youtube(youtube_url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T02:09:57.908784Z",
     "start_time": "2023-09-03T02:09:57.902120Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can query it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import query"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T02:09:57.912524Z",
     "start_time": "2023-09-03T02:09:57.910543Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "response = query.generate_on_summary(db=db, prompt=\"In bullet points, tell me what this material describes\", object_path=youtube_url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T02:10:03.494616Z",
     "start_time": "2023-09-03T02:09:57.913877Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The material is part of a series discussing contextual representations, specifically focusing on the GPT transformer-based architecture.\n",
      "- It covers topics such as autoregressive loss function, token representation, hidden representation, language modeling, transformer architecture, and masking.\n",
      "- The concept of self-attention and its role in allowing the model to look back at previous positions in a sequence when making predictions is explained.\n",
      "- The training process for a GPT-style model using \"teacher forcing\" is discussed, highlighting its significance.\n",
      "- The material briefly touches on the process of sequence generation and different strategies for sampling tokens.\n",
      "- It mentions that there are different versions of GPT and alternative models available.\n",
      "- The information provided is based on the text and may vary or become outdated over time.\n"
     ]
    }
   ],
   "source": [
    "print(response.generated_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T02:10:03.495201Z",
     "start_time": "2023-09-03T02:10:03.490884Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "response = query.generate_on_search(\n",
    "    db=db,\n",
    "    prompt=\"In bullet points, tell me what this material describes\",\n",
    "    search_query=\"open source models\",\n",
    "    object_path=youtube_url,\n",
    "    limit=2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T02:10:06.257604Z",
     "start_time": "2023-09-03T02:10:03.497199Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The material describes a summary of open alternatives in the field of open source.\n",
      "- It mentions that the information provided may be outdated.\n",
      "- It highlights the variety of models available in the open source community.\n",
      "- It mentions the Bloom model, which has 176 billion parameters and is considered extremely large.\n"
     ]
    }
   ],
   "source": [
    "print(response.generated_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T02:10:06.264306Z",
     "start_time": "2023-09-03T02:10:06.259962Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arxiv example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "159"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_url = 'https://arxiv.org/pdf/1706.03762'\n",
    "db.add_pdf(pdf_url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T02:10:06.274561Z",
     "start_time": "2023-09-03T02:10:06.265269Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The material describes a research paper titled \"Attention Is All You Need\" that introduces a new network architecture called the Transformer. The paper discusses the Transformer's use of attention mechanisms instead of recurrent or convolutional neural networks, its superior performance in tasks like machine translation and English constituency parsing, its requirement of less training time, and its high parallelizability. The paper covers various aspects of the Transformer model, including its architecture, comparison to existing models, performance in machine translation tasks, generalizability to other tasks, explanation of attention mechanisms, and researchers' contributions. It also discusses topics such as positional encoding, self-attention, and their comparison to recurrent and convolutional layers. The training process for models using the Transformer architecture is described, including the dataset used, encoding method, vocabulary size, batch formation, optimizer, regularization techniques, and results. The material highlights the performance and effectiveness of the Transformer model in machine translation and English constituency parsing tasks.\n"
     ]
    }
   ],
   "source": [
    "import query\n",
    "response = query.generate_on_summary(db=db, prompt=\"Tell me what this material describes\", object_path=pdf_url)\n",
    "print(response.generated_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T02:10:12.912423Z",
     "start_time": "2023-09-03T02:10:06.274312Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Attention heads are a component of the transformer model used in natural language processing tasks.\n",
      "- They are responsible for focusing on different parts of the input sequence during the encoding process.\n",
      "- Each attention head learns to attend to different aspects of the input, allowing the model to capture different types of information.\n",
      "- The attention heads operate in parallel, allowing for multiple perspectives to be considered simultaneously.\n",
      "- The attention heads compute a compatibility function between a query and a set of key-value pairs to determine the importance of each value.\n",
      "- The weights assigned to each value are computed using a softmax function, resulting in a distribution of attention weights.\n",
      "- The attention weights are then used to compute a weighted sum of the values, which is the output of the attention head.\n",
      "- By having multiple attention heads, the model can capture different types of dependencies and relationships in the input sequence.\n",
      "- The number of attention heads is a hyperparameter that can be tuned to balance model performance and computational efficiency.\n",
      "- Too few attention heads may limit the model's ability to capture complex patterns, while too many attention heads may lead to overfitting or increased computational cost.\n"
     ]
    }
   ],
   "source": [
    "response = query.generate_on_search(\n",
    "    db=db,\n",
    "    prompt=\"In bullet points, tell me how attention heads work\",\n",
    "    search_query=\"attention head\",\n",
    "    object_path=pdf_url,\n",
    "    limit=5\n",
    ")\n",
    "print(response.generated_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T02:10:21.318435Z",
     "start_time": "2023-09-03T02:10:12.913262Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T02:10:21.321116Z",
     "start_time": "2023-09-03T02:10:21.318897Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
