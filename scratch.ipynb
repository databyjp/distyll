{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-14T22:36:44.845419Z",
     "start_time": "2023-07-14T22:36:44.231897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new class:\n"
     ]
    }
   ],
   "source": [
    "import wkb\n",
    "client = wkb.start_db()\n",
    "collection = wkb.Collection(client, wkb.WV_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# collection.client.schema.delete_all()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T22:35:38.629557Z",
     "start_time": "2023-07-14T22:35:38.567372Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# youtube_url = 'https://youtu.be/xk28RMhRy1U'\n",
    "# youtube_url = 'https://youtu.be/FU7l5pr2FmU'\n",
    "# youtube_url = 'https://youtu.be/H39Z_720T5s'\n",
    "# youtube_url = 'https://youtu.be/wi63uvjs6Uc'  # God of war review\n",
    "youtube_url = 'https://youtu.be/zN4VCb0LbQI'  # Pydnatic vs dataclasses vs attrs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T22:36:49.523922Z",
     "start_time": "2023-07-14T22:36:49.520141Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/zN4VCb0LbQI\n",
      "[youtube] zN4VCb0LbQI: Downloading webpage\n",
      "[youtube] zN4VCb0LbQI: Downloading ios player API JSON\n",
      "[youtube] zN4VCb0LbQI: Downloading android player API JSON\n",
      "[youtube] zN4VCb0LbQI: Downloading m3u8 information\n",
      "[info] zN4VCb0LbQI: Downloading 1 format(s): 251\n",
      "[download] Destination: temp_audio.mp3\n",
      "[download] 100% of   13.10MiB in 00:00:01 at 10.90MiB/s  \n",
      "Found Attrs, Pydantic, or Python Data Classes? - downloading\n",
      "[youtube] Extracting URL: https://youtu.be/zN4VCb0LbQI\n",
      "[youtube] zN4VCb0LbQI: Downloading webpage\n",
      "[youtube] zN4VCb0LbQI: Downloading ios player API JSON\n",
      "[youtube] zN4VCb0LbQI: Downloading android player API JSON\n",
      "[youtube] zN4VCb0LbQI: Downloading m3u8 information\n",
      "[info] zN4VCb0LbQI: Downloading 1 format(s): 251\n",
      "[download] temp_audio.mp3 has already been downloaded\n",
      "[download] 100% of   13.10MiB\n",
      "Successfully Downloaded to temp_audio.mp3\n",
      "Splitting audio to 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jphwang/code/sandpit/wv_simple_kb/utils.py:105: ResourceWarning: unclosed file <_io.BufferedRandom name='0_temp_audio.mp3'>\n",
      "  clip.export(f\"{i}_\" + audio_file_path)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/jphwang/code/sandpit/wv_simple_kb/utils.py:105: ResourceWarning: unclosed file <_io.BufferedRandom name='1_temp_audio.mp3'>\n",
      "  clip.export(f\"{i}_\" + audio_file_path)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting transcripts from 2 audio files...\n",
      "Processing transcript 1 of 2...\n",
      "Processing transcript 2 of 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jphwang/code/sandpit/wv_simple_kb/utils.py:71: ResourceWarning: unclosed file <_io.BufferedReader name='0_temp_audio.mp3'>\n",
      "  audio_file = open(clip_outpath, \"rb\")\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/zN4VCb0LbQI\n",
      "[youtube] zN4VCb0LbQI: Downloading webpage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jphwang/code/sandpit/wv_simple_kb/wkb.py:155: ResourceWarning: unclosed file <_io.BufferedReader name='1_temp_audio.mp3'>\n",
      "  transcript_texts = _get_transcripts_from_audio_file(outpath)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] zN4VCb0LbQI: Downloading ios player API JSON\n",
      "[youtube] zN4VCb0LbQI: Downloading android player API JSON\n",
      "[youtube] zN4VCb0LbQI: Downloading m3u8 information\n",
      "[youtube] Extracting URL: https://youtu.be/zN4VCb0LbQI\n",
      "[youtube] zN4VCb0LbQI: Downloading webpage\n",
      "[youtube] zN4VCb0LbQI: Downloading ios player API JSON\n",
      "[youtube] zN4VCb0LbQI: Downloading android player API JSON\n",
      "[youtube] zN4VCb0LbQI: Downloading m3u8 information\n"
     ]
    },
    {
     "data": {
      "text/plain": "35"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.add_from_youtube(youtube_url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T22:38:39.882682Z",
     "start_time": "2023-07-14T22:36:51.718433Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé•üíª Summary: This video compares Python packages Atters, Pydantic, and Dataclasses. Topics include precision in currency values, object comparison, validation options, and pros/cons. Learn about choosing the right package for your project! #Python #DataValidation\n",
      "\n",
      "üóíÔ∏èüí° Summary: Discover the best ways to validate data in Python! This video explores Pydantic, Atters, and Dataclasses. Topics include attribute validation, built-in vs manual validation, and pros/cons of each package. Choose wisely for your project! #Python #DataValidation\n"
     ]
    }
   ],
   "source": [
    "print(collection.summarize_entry(youtube_url, custom_prompt=\"Summarize this video in two tweets - make it interesting and short, with emojis\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T22:40:27.474503Z",
     "start_time": "2023-07-14T22:40:00.120557Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé•üêç In a video, they compare Python data class alternatives‚öñÔ∏è\n",
      "üíºüì¶ Pydantic: powerful data validation & conversionüîí\n",
      "üèóÔ∏è Atters: better control over defs, validation & comparisonüî©\n",
      "üì¶ Dataclasses: basics without advanced validation or comparison‚öôÔ∏è\n",
      "üìö They recommend considering needs for project excellenceüåü\n",
      "üîó Watch the vid to learn about the best option for you‚ú®\n"
     ]
    }
   ],
   "source": [
    "print(collection.summarize_entry(youtube_url, custom_prompt=\"Summarize their conclusions about Pydantic, dataclasses and Attrs in a tweet or two - use emojis and make it interesting.\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T22:15:32.216725Z",
     "start_time": "2023-07-14T22:15:06.701458Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This video covers the topic of transformers in machine learning. Transformers are a type of neural network architecture that has multiple applications, including translating text, generating computer code, and solving complex problems like protein folding. The video explains the limitations of recurrent neural networks (RNNs) in analyzing language and introduces transformers as a more efficient and powerful alternative. It highlights the key innovations of positional encodings and attention, particularly self-attention, that make transformers effective.\n",
      "\n",
      "The video also mentions popular transformer-based models like BERT and their applications in natural language processing tasks such as text summarization, question answering, and classification. By watching the video, the viewer can learn about the concept of transformers, their advantages over RNNs, and their practical applications in various domains.\n",
      "\n",
      "In addition to transformers, the video covers several topics related to neural networks and language understanding. It discusses the importance of aligning words in different languages for translation and explains how neural networks, particularly transformers, can automatically build an understanding of language. The video introduces the concept of attention in neural networks, which allows the model to focus on specific words in the input sentence when making predictions. It highlights the use of self-attention, a twist on traditional attention, to help the model understand words in the context of surrounding words. The video emphasizes the value of self-attention in disambiguating words, recognizing parts of speech, and identifying word tense.\n",
      "\n",
      "Overall, the video provides insights into how neural networks learn language and the role of attention and self-attention in language understanding. The viewer can gain knowledge about transformers, their applications, and the benefits they offer over traditional RNNs. Additionally, the video offers resources for accessing pre-trained transformer models.\n"
     ]
    }
   ],
   "source": [
    "print(collection.summarize_entry(youtube_url))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T21:55:19.743208Z",
     "start_time": "2023-07-14T21:54:57.576980Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main topic of the videos is transformers in machine learning and their applications in language processing. The related sub-topics that the user might learn about include:\n",
      "\n",
      "1. Limitations of recurrent neural networks (RNNs) in analyzing language\n",
      "2. Key innovations of positional encodings and attention in transformers\n",
      "3. Introduction to self-attention and its role in understanding language\n",
      "4. Popular transformer-based models like BERT and their applications in natural language processing tasks\n",
      "5. Importance of aligning words in different languages for translation\n",
      "6. Role of attention in neural networks and its ability to focus on specific words in the input sentence\n",
      "\n",
      "These sub-topics provide a comprehensive understanding of transformers, their advantages over RNNs, and their practical applications in language understanding and processing.\n"
     ]
    }
   ],
   "source": [
    "topic_prompt = f\"\"\"\n",
    "Extract a list of three to six related sub-topics\n",
    "related to the main topic that the user might learn about.\n",
    "Deliver the topics as a short list, each separated by two consecutive newlines like `\\n\\n`\n",
    "\"\"\"\n",
    "print(collection.summarize_entry(youtube_url, custom_prompt=topic_prompt))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T21:56:12.808914Z",
     "start_time": "2023-07-14T21:55:56.064939Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 1: üì∫ü§ñ Watch this video to learn about transformers in machine learning! Discover how they revolutionize language analysis and translation, from writing poems to solving complex problems like protein folding. üí°üåê #MachineLearning #Transformers\n",
      "\n",
      "Tweet 2: üé•üîç Dive into the fascinating world of neural networks and language understanding with this video! Explore the power of attention and self-attention in building an internal representation of language, disambiguating words, and recognizing parts of speech. üß†üó£Ô∏è #NeuralNetworks #LanguageUnderstanding\n"
     ]
    }
   ],
   "source": [
    "print(collection.summarize_entry(youtube_url, f\"Summarize what I learned about the topic in two tweets to pique the readers' interest and encourage them to watch the video, like 'I learned xxx from this video - you could learn xxx from this video'. The tweets should be succinct and fun without being hyperbolic and refrain from being written like too marketing-like. Please, still use emojis and make it interesting\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T22:06:20.886944Z",
     "start_time": "2023-07-14T22:06:05.859752Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning words in different languages is crucial for translation because language is the primary means of human communication. In traditional models like recurrent neural networks (RNNs), words are processed sequentially, and their order matters. For example, shuffling the words in a sentence can completely change its meaning. However, RNNs have limitations in handling large sequences of text and are difficult to train.\n",
      "\n",
      "Transformers, on the other hand, address these issues with three key innovations: positional encodings, attention, and self-attention. Positional encodings assign a number to each word in a sentence, capturing word order in the data itself. This allows the neural network to learn the importance of word order during training.\n",
      "\n",
      "Attention is a mechanism that enables the model to consider every word in the original sentence when making translation decisions. It allows the model to align words in the input and output sentences, considering factors like word order and gender agreement. Attention is learned from data, enabling the model to understand grammar and language rules.\n",
      "\n",
      "Self-attention is a variation of attention that helps the model understand the underlying meaning in language. As the model analyzes vast amounts of text data, it builds an internal representation or understanding of language. Self-attention allows the model to consider the context of surrounding words, disambiguating word meanings, recognizing parts of speech, and identifying word tense.\n",
      "\n",
      "By aligning words in different languages, transformers can accurately translate text and perform various language tasks. They provide a more effective and flexible approach to language understanding and processing.\n"
     ]
    }
   ],
   "source": [
    "print(collection.ask_object(youtube_url, \"Importance of aligning words in different languages for translation\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T22:05:27.787210Z",
     "start_time": "2023-07-14T22:05:18.408587Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
