{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-14T21:07:38.285827Z",
     "start_time": "2023-07-14T21:07:37.631687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new class:\n"
     ]
    }
   ],
   "source": [
    "import wkb\n",
    "client = wkb.start_db()\n",
    "collection = wkb.Collection(client, wkb.WV_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# collection.client.schema.delete_all()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T11:12:21.110505Z",
     "start_time": "2023-07-13T11:12:21.107339Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# youtube_url = 'https://youtu.be/xk28RMhRy1U'\n",
    "# youtube_url = 'https://youtu.be/FU7l5pr2FmU'\n",
    "youtube_url = 'https://youtu.be/H39Z_720T5s'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T11:24:11.804557Z",
     "start_time": "2023-07-13T11:24:11.799721Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/H39Z_720T5s\n",
      "[youtube] H39Z_720T5s: Downloading webpage\n",
      "[youtube] H39Z_720T5s: Downloading ios player API JSON\n",
      "[youtube] H39Z_720T5s: Downloading android player API JSON\n",
      "[youtube] H39Z_720T5s: Downloading m3u8 information\n",
      "[info] H39Z_720T5s: Downloading 1 format(s): 251\n",
      "[download] Destination: temp_audio.mp3\n",
      "[download] 100% of    1.93MiB in 00:00:00 at 3.01MiB/s   \n",
      "Found The Transformer architecture - downloading\n",
      "[youtube] Extracting URL: https://youtu.be/H39Z_720T5s\n",
      "[youtube] H39Z_720T5s: Downloading webpage\n",
      "[youtube] H39Z_720T5s: Downloading ios player API JSON\n",
      "[youtube] H39Z_720T5s: Downloading android player API JSON\n",
      "[youtube] H39Z_720T5s: Downloading m3u8 information\n",
      "[info] H39Z_720T5s: Downloading 1 format(s): 251\n",
      "[download] temp_audio.mp3 has already been downloaded\n",
      "[download] 100% of    1.93MiB\n",
      "Successfully Downloaded to temp_audio.mp3\n",
      "Audio file under 900 seconds. No split required.\n",
      "Getting transcripts from 1 audio files...\n",
      "Processing transcript 1 of 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jphwang/code/sandpit/wv_simple_kb/wkb.py:150: ResourceWarning: unclosed file <_io.BufferedReader name='temp_audio.mp3'>\n",
      "  transcript_texts = _get_transcripts_from_audio_file(outpath)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.add_from_youtube(youtube_url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T11:24:36.619138Z",
     "start_time": "2023-07-13T11:24:12.474155Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video covers the topic of encoders-decoders and the transformer architecture. It explains that the encoder and decoder are two components of this architecture that can be used together or independently. The encoder is responsible for converting text into numerical representations using the self-attention mechanism, while the decoder uses a similar mechanism but with unidirectional features. The video suggests watching specific videos on encoders and decoders to gain a more comprehensive understanding of these concepts. It also introduces the concept of an encoder-decoder, which combines both parts to generate predictions.\n",
      "\n",
      "By listening to this video, the reader can learn about the fundamentals of encoders-decoders and the transformer architecture. They will gain insights into how these components are utilized in natural language processing tasks. Overall, the video provides an overview of the encoder and decoder and their respective functions, as well as the benefits of combining them in an encoder-decoder setting.\n"
     ]
    }
   ],
   "source": [
    "print(collection.summarize_entry(youtube_url))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T11:26:47.964463Z",
     "start_time": "2023-07-13T11:26:38.359905Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Transformers, encoders, decoders, oh my! ðŸ¤– Get the lowdown on the basics of these key components in natural language processing. ðŸŽ“ Watch this video to understand how they work and how they can level up your NLP game! #NLP #Transformers\"\n",
      "\n",
      "\"Ever wondered how text can be transformed into numerical representations for language processing? ðŸ“šðŸ”¢ Look no further! This video breaks down the encoder-decoder relationship and the self-attention mechanism in the transformer architecture. Watch it to unlock the secrets of NLP! #NLP #Transformers\"\n"
     ]
    }
   ],
   "source": [
    "print(collection.summarize_entry(youtube_url, \"Summarize what I learned about the covered topic in two tweets to pique the readers' interest and encourage them to watch the video, like 'I learned xxx from this video - you could learn xxx from this video'. The tweets should be succinct and fun without being hyperbolic and refrain from being written like too marketing-like.\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T11:28:20.016319Z",
     "start_time": "2023-07-13T11:28:10.914253Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA stands for Low Rank Adaptation. It is a technique used in machine learning to improve the performance of pre-trained models during adaptation. The key idea behind LoRA is that pre-trained models have low intrinsic rank, meaning they can be accurately represented using smaller matrices. By decomposing the weight update matrix into smaller matrices, LoRA allows for more efficient fine-tuning of the model. This results in significant memory savings and faster processing speed. Additionally, LoRA allows for easy swapping between different tasks without imposing a significant inference penalty. Overall, LoRA is a powerful tool for optimizing and adapting pre-trained models in machine learning. #LoRA #MachineLearning #Optimization\n"
     ]
    }
   ],
   "source": [
    "print(collection.ask_object(youtube_url, \"LoRA\", \"What is LoRA and why is it useful? Explain in a tweet or so in plain, relatable language\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T11:16:11.257805Z",
     "start_time": "2023-07-13T11:16:06.636010Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
