{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Workshop demo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 6666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Shutting down... \",\"time\":\"2023-07-20T09:21:30+01:00\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Stopped serving weaviate at http://127.0.0.1:6666\",\"time\":\"2023-07-20T09:21:30+01:00\"}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import weaviate\n",
    "from weaviate import EmbeddedOptions\n",
    "\n",
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions(version=\"1.20.2\"),\n",
    "    additional_headers={\"X-OpenAI-Api-Key\": os.environ[\"OPENAI_APIKEY\"]}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:21:30.660941Z",
     "start_time": "2023-07-20T08:21:30.550612Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded weaviate wasn't listening on port 6666, so starting embedded weaviate again\n",
      "Started /Users/jphwang/.cache/weaviate-embedded: process ID 16784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2023-07-20T09:21:34+01:00\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2023-07-20T09:21:34+01:00\"}\n",
      "{\"level\":\"warning\",\"msg\":\"Multiple vector spaces are present, GraphQL Explore and REST API list objects endpoint module include params has been disabled as a result.\",\"time\":\"2023-07-20T09:21:34+01:00\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50051\",\"time\":\"2023-07-20T09:21:34+01:00\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:6666\",\"time\":\"2023-07-20T09:21:34+01:00\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.is_ready()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:21:34.727267Z",
     "start_time": "2023-07-20T08:21:34.484825Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class_definition = {\n",
    "    \"class\": \"Chunk\",\n",
    "    \"vectorizer\": \"text2vec-openai\",\n",
    "    \"moduleConfig\": {\n",
    "        \"generative-openai\": {}\n",
    "    },\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:39:53.538474Z",
     "start_time": "2023-07-20T08:39:53.535435Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"chunk_C4fZXCMGYgeG\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-07-20T09:39:55+01:00\",\"took\":57042}\n"
     ]
    }
   ],
   "source": [
    "client.schema.create_class(class_definition)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:39:55.009760Z",
     "start_time": "2023-07-20T08:39:54.984821Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "{'class': 'Chunk',\n 'invertedIndexConfig': {'bm25': {'b': 0.75, 'k1': 1.2},\n  'cleanupIntervalSeconds': 60,\n  'stopwords': {'additions': None, 'preset': 'en', 'removals': None}},\n 'moduleConfig': {'generative-openai': {},\n  'text2vec-openai': {'model': 'ada',\n   'modelVersion': '002',\n   'type': 'text',\n   'vectorizeClassName': True}},\n 'multiTenancyConfig': {'enabled': False},\n 'properties': [],\n 'replicationConfig': {'factor': 1},\n 'shardingConfig': {'virtualPerPhysical': 128,\n  'desiredCount': 1,\n  'actualCount': 1,\n  'desiredVirtualCount': 128,\n  'actualVirtualCount': 128,\n  'key': '_id',\n  'strategy': 'hash',\n  'function': 'murmur3'},\n 'vectorIndexConfig': {'skip': False,\n  'cleanupIntervalSeconds': 300,\n  'maxConnections': 64,\n  'efConstruction': 128,\n  'ef': -1,\n  'dynamicEfMin': 100,\n  'dynamicEfMax': 500,\n  'dynamicEfFactor': 8,\n  'vectorCacheMaxObjects': 1000000000000,\n  'flatSearchCutoff': 40000,\n  'distance': 'cosine',\n  'pq': {'enabled': False,\n   'bitCompression': False,\n   'segments': 0,\n   'centroids': 256,\n   'trainingLimit': 100000,\n   'encoder': {'type': 'kmeans', 'distribution': 'log-normal'}}},\n 'vectorIndexType': 'hnsw',\n 'vectorizer': 'text2vec-openai'}"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.schema.get(\"Chunk\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:39:56.459301Z",
     "start_time": "2023-07-20T08:39:56.451395Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "'A large language model (LLM) is a computerized language model, embodied by an artificial neural netw'"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"srcdata/llm_wiki.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:39:58.367779Z",
     "start_time": "2023-07-20T08:39:58.362180Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "39108"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:39:58.981532Z",
     "start_time": "2023-07-20T08:39:58.977640Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "7206"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split(\" \"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:39:59.431842Z",
     "start_time": "2023-07-20T08:39:59.428604Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model (LLM) is a computerized lan ...\n",
      "=====\n",
      "\n",
      "History\n",
      "Precursors\n",
      "The basic idea of LLMs, which i ...\n",
      "=====\n",
      "\n",
      "Lead-up to the transformer framework\n",
      "The earliest  ...\n",
      "=====\n",
      "\n",
      "The seq2seq model (380 million parameters) used tw ...\n",
      "=====\n",
      "\n",
      "BERT and GPT\n",
      "While there are many models with diff ...\n",
      "=====\n",
      "\n",
      "Origin of the term and disambiguation\n",
      "While the te ...\n",
      "=====\n",
      "\n",
      "Linguistic foundations\n",
      "Cognitive Linguistics offer ...\n",
      "=====\n",
      "\n",
      "Architecture\n",
      "Large language models have most commo ...\n",
      "=====\n",
      "\n",
      "Tokenizers, which convert text into machine-readab ...\n",
      "=====\n",
      "\n",
      "Tokenization\n",
      "LLMs are mathematical functions whose ...\n",
      "=====\n",
      "\n",
      "Output\n",
      "The output of a LLM is a probability distri ...\n",
      "=====\n",
      "\n",
      "Upon receiving a text, the bulk of the LLM outputs ...\n",
      "=====\n",
      "\n",
      "Context window\n",
      "The context window of a LLM is the  ...\n",
      "=====\n",
      "\n",
      "Training\n",
      "In the pre-training, LLMs may be trained  ...\n",
      "=====\n",
      "\n",
      "autoregressive (i.e. predicting how the segment co ...\n",
      "=====\n",
      "\n",
      "Dataset size and compression\n",
      "In 2018, the BookCorp ...\n",
      "=====\n",
      "\n",
      "Training cost\n",
      "Advances in software and hardware ha ...\n",
      "=====\n",
      "\n",
      "Application to downstream tasks\n",
      "Between 2018 and 2 ...\n",
      "=====\n",
      "\n",
      "From fine-tuning to prompting\n",
      "The old approach was ...\n",
      "=====\n",
      "\n",
      "Review: This movie is fantastic!\n",
      "Sentiment: ...\n",
      "=====\n",
      "\n",
      "If the model outputs \"positive\", then it has corre ...\n",
      "=====\n",
      "\n",
      "Instruction tuning\n",
      "Often, instruction tuning is ne ...\n",
      "=====\n",
      "\n",
      "Finetuning by reinforcement learning\n",
      "OpenAI's Inst ...\n",
      "=====\n",
      "\n",
      "Tool use\n",
      "There are certain tasks that, in principl ...\n",
      "=====\n",
      "\n",
      "Agency\n",
      "An LLM is a language model, which is not an ...\n",
      "=====\n",
      "\n",
      "Multimodality\n",
      "Multimodality means \"having several  ...\n",
      "=====\n",
      "\n",
      "Properties\n",
      "Pretraining datasets\n",
      "Large language mod ...\n",
      "=====\n",
      "\n",
      "Scaling laws and emergent abilities\n",
      "The following  ...\n",
      "=====\n",
      "\n",
      "cost of (pre-)training (\n",
      "  \n",
      "    C\n",
      "    C\n",
      "  ),\n",
      "size  ...\n",
      "=====\n",
      "\n",
      "  \n",
      "    C\n",
      "    C\n",
      "   is the cost of training the mode ...\n",
      "=====\n",
      "\n",
      "  \n",
      "    N\n",
      "    N\n",
      "   is the number of parameters in t ...\n",
      "=====\n",
      "\n",
      "  \n",
      "    D\n",
      "    D\n",
      "   is the number of tokens in the t ...\n",
      "=====\n",
      "\n",
      "  \n",
      "    L\n",
      "    L\n",
      "   is the average negative log-like ...\n",
      "=====\n",
      "\n",
      "  \n",
      "    \n",
      "      \n",
      "        \n",
      "          C\n",
      "          \n",
      "    ...\n",
      "=====\n",
      "\n",
      "  \n",
      "    \n",
      "      \n",
      "        α\n",
      "        =\n",
      "        0.34\n",
      "   ...\n",
      "=====\n",
      "\n",
      "reported arithmetics, decoding the International P ...\n",
      "=====\n",
      "\n",
      "Interpretation\n",
      "Large language models by themselves ...\n",
      "=====\n",
      "\n",
      "Understanding and intelligence\n",
      "NLP researchers wer ...\n",
      "=====\n",
      "\n",
      "Evaluation\n",
      "Perplexity\n",
      "The most commonly used measu ...\n",
      "=====\n",
      "\n",
      "Task-specific datasets and benchmarks\n",
      "A large numb ...\n",
      "=====\n",
      "\n",
      "Adversarially constructed evaluations\n",
      "Because of t ...\n",
      "=====\n",
      "\n",
      "We see a fitness center sign. We then see a man ta ...\n",
      "=====\n",
      "\n",
      "BERT selects b) as the most likely completion, tho ...\n",
      "=====\n",
      "\n",
      "Wider impact\n",
      "In 2023, Nature Biomedical Engineerin ...\n",
      "=====\n",
      "\n",
      "List\n",
      "Further reading\n",
      "Jurafsky, Dan, Martin, James. ...\n",
      "=====\n",
      "\n",
      "See also\n",
      "Foundation models\n",
      "Generative AI ...\n",
      "=====\n",
      "\n",
      "Notes ...\n",
      "=====\n",
      "\n",
      "\n",
      "== References == ...\n",
      "=====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = text.split(\"\\n\\n\")\n",
    "for chunk in chunks:\n",
    "    print(chunk[:50], \"...\\n=====\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:39:59.951102Z",
     "start_time": "2023-07-20T08:39:59.947516Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "with client.batch() as batch:\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        weaviate_object = {\n",
    "            \"body\": chunk,\n",
    "            \"chunk_no\": i,\n",
    "            \"source\": \"llm_wiki.txt\",\n",
    "        }\n",
    "        batch.add_data_object(\n",
    "            class_name=\"Chunk\",\n",
    "            data_object=weaviate_object,\n",
    "            uuid=generate_uuid5(weaviate_object)\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:40:03.566486Z",
     "start_time": "2023-07-20T08:40:00.676774Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "{'data': {'Aggregate': {'Chunk': [{'meta': {'count': 48}}]}}}"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.query.aggregate(\"Chunk\").with_meta_count().do()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:40:04.254802Z",
     "start_time": "2023-07-20T08:40:04.248855Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "48"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:40:04.981768Z",
     "start_time": "2023-07-20T08:40:04.976584Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "res = (\n",
    "    client.query.get(\"Chunk\", [\"body\", \"chunk_no\"])\n",
    "    .with_near_text(\n",
    "        {\"concepts\": [\"history of large language models\"]}\n",
    "    )\n",
    "    .with_limit(3)\n",
    "    .do()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:40:06.568651Z",
     "start_time": "2023-07-20T08:40:05.783334Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": {\n",
      "    \"Get\": {\n",
      "      \"Chunk\": [\n",
      "        {\n",
      "          \"body\": \"Lead-up to the transformer framework\\nThe earliest \\\"large\\\" language models were built with recurrent architectures such as the long short-term memory (LSTM) (1997). After AlexNet (2012) demonstrated the effectiveness of large neural networks for image recognition, researchers applied large neural networks to other tasks. In 2014, two main techniques were proposed.\",\n",
      "          \"chunk_no\": 2\n",
      "        },\n",
      "        {\n",
      "          \"body\": \"Origin of the term and disambiguation\\nWhile the term of Large Language Models has itself emerged around 2018, it gained visibility in 2019 and 2020, with the release of DistilBERT and Stochastic Parrots papers respectively. Both focused on the \\\"Large-scale pretrained models\\\", citing as an example of LLMs the BERT family, starting at 110M parameters and referring to models in the 340M parameters range as \\\"very large LMs\\\".\\nPerhaps surprisingly, both cite the pre-transformer RNN-based ELMo - the 2018 architecture that inspired BERT - as the first LLM, given the number of parameters (94M), as well as the size of the pretraining dataset (>1B tokens). Despite the comparable parameter size, the original Transformer is generally not considered as an LLM due to a smaller pretraining dataset (generally estimated in the 100M tokens range).\\nOverall, due to a smooth scaling in LLM model performance from  ~100M parameters to 500B+ parameters and progressive unlocking of emergent capabilities such as multi-lingual translation, arithmetic, or programming code composition, all post-ELMo models are referred to by researchers as LLMs.\",\n",
      "          \"chunk_no\": 5\n",
      "        },\n",
      "        {\n",
      "          \"body\": \"List\\nFurther reading\\nJurafsky, Dan, Martin, James. H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 3rd Edition draft, 2023.\\nPhuong, Mary; Hutter, Marcus (2022). \\\"Formal Algorithms for Transformers\\\". arXiv:2207.09238 [cs.LG].\\nEloundou, Tyna; Manning, Sam; Mishkin, Pamela; Rock, Daniel (2023). \\\"GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models\\\". arXiv:2303.10130 [econ.GN].\\nEldan, Ronen; Li, Yuanzhi (2023). \\\"TinyStories: How Small Can Language Models Be and Still Speak Coherent English?\\\". arXiv:2305.07759 [cs.CL].\\nFrank, Michael C. (27 June 2023). \\\"Baby steps in evaluating the capacities of large language models\\\". Nature Reviews Psychology: 1\\u20132. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023.\\nZhao, Wayne Xin;  et al. (2023). \\\"A Survey of Large Language Models\\\". arXiv:2303.18223 [cs.CL].\",\n",
      "          \"chunk_no\": 44\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(res, indent=2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:40:08.160344Z",
     "start_time": "2023-07-20T08:40:08.150397Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "res = (\n",
    "    client.query.get(\"Chunk\", [\"body\", \"chunk_no\"])\n",
    "    .with_near_text(\n",
    "        {\"concepts\": [\"history\"]}\n",
    "    )\n",
    "    .with_limit(5)\n",
    "    .with_generate(\n",
    "        grouped_task=\"Explain the history of large language models in plain language, based on this text\"\n",
    "    )\n",
    "    .do()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:43:53.023180Z",
     "start_time": "2023-07-20T08:43:39.522173Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "{'data': {'Get': {'Chunk': [{'_additional': {'generate': {'error': None,\n       'groupedResult': 'Large language models (LLMs) have a history that dates back to the 1950s. However, it was not until the 2010s that they became feasible due to the use of GPUs (graphics processing units) for massively parallelized processing. Before this, the idea of using a simple repetitive architecture to train a neural network on a large language corpus remained just an idea.\\n\\nOne of the precursors to LLMs was the Elman network, which was trained on simple sentences like \"dog chases man.\" The trained model was then used to convert each word into a vector, which represented its internal representation. These vectors were clustered based on their closeness, forming a tree-like structure. Within this structure, verbs and nouns belonged to one large cluster, and within the noun cluster, there were further clusters for inanimates and animates.\\n\\nIn the 1990s, IBM alignment models for statistical machine translation hinted at the future success of LLMs. However, it was not until 2001 that an early work using a corpus scraped from the internet for word disambiguation showcased the potential of LLMs. This work utilized a 1-billion-word corpus, which was considered huge at that time.\\n\\nOverall, the history of LLMs involves the gradual development of techniques and technologies, such as the use of GPUs, to enable the training of neural networks on large language corpora. These advancements have paved the way for the impressive capabilities of modern LLMs.'}},\n     'body': '\\n== References ==',\n     'chunk_no': 47},\n    {'_additional': {'generate': None},\n     'body': 'Review: This movie is fantastic!\\nSentiment:',\n     'chunk_no': 19},\n    {'_additional': {'generate': None}, 'body': 'Notes', 'chunk_no': 46},\n    {'_additional': {'generate': None},\n     'body': 'BERT selects b) as the most likely completion, though the correct answer is d).',\n     'chunk_no': 42},\n    {'_additional': {'generate': None},\n     'body': 'History\\nPrecursors\\nThe basic idea of LLMs, which is to start with a neural network as black box with randomized weights, using a simple repetitive architecture and (pre-)training it on a large language corpus, was not feasible until the 2010s when use of GPUs had enabled massively parallelized processing, which has gradually replaced the logical AI approach that has relied on symbolic programs.Precursors of LLMs included the Elman network, in which a recurrent network was trained on simple sentences like \"dog chases man\". Then, the (pre-)trained model was used to convert each word into a vector (its \\'internal representation\\'). These vectors were clustered by closeness into a tree. The tree was then found to have a structure. The verbs and nouns each belonged to one large cluster. Within the noun cluster, there are two clusters: inanimates and animates. And so on.\\nIn the 1950s, without the modern GPUs enabling massively parallel processing, the idea to learn natural language by a simple repetitive architecture remained just an idea. Later in 1990s, the IBM alignment models for statistical machine translation announced the future success of LLMs. An early work that uses corpus scraped from the Internet for word disambiguation (such as distinguishing \"then\" and \"than\") in 2001. It used a 1-billion-word corpus, considered huge at the time.',\n     'chunk_no': 1}]}}}"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:44:07.377882Z",
     "start_time": "2023-07-20T08:44:07.373804Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "'Large language models (LLMs) have a history that dates back to the 1950s. However, it was not until the 2010s that they became feasible due to the use of GPUs (graphics processing units) for massively parallelized processing. Before this, the idea of using a simple repetitive architecture to train a neural network on a large language corpus remained just an idea.\\n\\nOne of the precursors to LLMs was the Elman network, which was trained on simple sentences like \"dog chases man.\" The trained model was then used to convert each word into a vector, which represented its internal representation. These vectors were clustered based on their closeness, forming a tree-like structure. Within this structure, verbs and nouns belonged to one large cluster, and within the noun cluster, there were further clusters for inanimates and animates.\\n\\nIn the 1990s, IBM alignment models for statistical machine translation hinted at the future success of LLMs. However, it was not until 2001 that an early work using a corpus scraped from the internet for word disambiguation showcased the potential of LLMs. This work utilized a 1-billion-word corpus, which was considered huge at that time.\\n\\nOverall, the history of LLMs involves the gradual development of techniques and technologies, such as the use of GPUs, to enable the training of neural networks on large language corpora. These advancements have paved the way for the impressive capabilities of modern LLMs.'"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"data\"][\"Get\"][\"Chunk\"][0][\"_additional\"][\"generate\"][\"groupedResult\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:44:09.164873Z",
     "start_time": "2023-07-20T08:44:09.160903Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "res = (\n",
    "    client.query.get(\"Chunk\", [\"body\", \"chunk_no\"])\n",
    "    .with_near_text(\n",
    "        {\"concepts\": [\"history\"]}\n",
    "    )\n",
    "    .with_limit(5)\n",
    "    .with_generate(\n",
    "        grouped_task=\"Explain the history of large language models in a tweet with emojis, based on the following text\"\n",
    "    )\n",
    "    .do()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:44:18.016726Z",
     "start_time": "2023-07-20T08:44:13.531960Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "'📚🔍 LLMs history: In the 1950s, the idea of using a simple repetitive architecture to learn natural language was just an idea. In the 1990s, IBM alignment models paved the way for LLMs. In the 2010s, GPUs enabled massively parallel processing, making LLMs feasible. 🖥️💡🔢🌐 #LLM #History'"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"data\"][\"Get\"][\"Chunk\"][0][\"_additional\"][\"generate\"][\"groupedResult\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-20T08:44:18.896933Z",
     "start_time": "2023-07-20T08:44:18.891143Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
